#version 450 core

layout(push_constant) uniform Block
{
	vec4 camPosClipmapExtent;
	uint lightCount; // Todo(achal): This needs to be activeLightCount
	uint buildHistogramID;
} pc;

#include <../cull_common.glsl>

#include <nbl/builtin/glsl/workgroup/arithmetic.glsl>

shared uint prevBlockSumShared;
shared uint cumHistogramShared[BIN_COUNT];

// lightPool
layout (set = 0, binding = 0, std430) restrict buffer readonly Lights
{
	nbl_glsl_ext_ClusteredLighting_SpotLight data[];
} lights;

// Todo(achal): Probably should make it a usamplerBuffer?
layout (set = 0, binding = 1, std430) restrict buffer readonly ActiveLightIndices
{
	uint data[];
} activeLightIndices;

layout (set = 0, binding = 2, std430) restrict writeonly buffer OutScratch
{
  uint count;
  uint padding;
  uvec2 data[];
} outScratch;

layout (set = 0, binding = 3, std430) restrict buffer ImportanceHistogram
{
	uint data[];
} importanceHistogram;

// Todo(achal): Bad names, don't know what they're for..
const uint lastSharedUint = _NBL_GLSL_WORKGROUP_SIZE_*2;
shared uint scratchShared[_NBL_GLSL_WORKGROUP_SIZE_*2+2];

uint getHistogramBinIndex(in float importanceMagnitude)
{
	const int minVal = floatBitsToInt(MIN_HISTOGRAM_IMPORTANCE) + 1;
	const int maxVal = floatBitsToInt(MAX_HISTOGRAM_IMPORTANCE) - 1;
	const int range = maxVal - minVal;

	return uint(BIN_COUNT * (float(clamp(floatBitsToInt(importanceMagnitude) - minVal, 0, range)) / float(range)));
}

void main()
{
	const uint buildHistogramOffset = pc.buildHistogramID*BIN_COUNT;
	const uint useHistogramOffset = (1u - pc.buildHistogramID)*BIN_COUNT;

	// build the histogram
	if (gl_GlobalInvocationID.x < pc.lightCount)
	{
		const uint globalLightIndex = activeLightIndices.data[gl_GlobalInvocationID.x];
		const nbl_glsl_ext_ClusteredLighting_SpotLight light = lights.data[globalLightIndex];
		const float importanceMagnitude = getLightImportanceMagnitude(light);
		const uint binIndex = getHistogramBinIndex(importanceMagnitude);
		atomicAdd(importanceHistogram.data[buildHistogramOffset + binIndex], 1u);
	}

	prevBlockSumShared = 0u;
	barrier();

	const uint stepCount = BIN_COUNT/_NBL_GLSL_WORKGROUP_SIZE_;
	for (uint step = 0; step < stepCount; ++step)
	{
		uint index = gl_LocalInvocationIndex + step*_NBL_GLSL_WORKGROUP_SIZE_;
		const uint scanData = importanceHistogram.data[useHistogramOffset + index];
		const uint incScanResult = nbl_glsl_workgroupInclusiveAdd(scanData);

		cumHistogramShared[index] = prevBlockSumShared + (incScanResult - scanData);

		if (gl_LocalInvocationIndex == (_NBL_GLSL_WORKGROUP_SIZE_ - 1u))
			prevBlockSumShared += incScanResult;
		barrier();
		memoryBarrierShared(); // this should make the write done by gl_LocalInvocationIndex (== _NBL_GLSL_WORKGROUP_SIZE_) visible to other invocations in the workgroup
	}

	// clear the (global memory) histogram after use for the next frame
	{
		const uint totalInvocationCount = _NBL_GLSL_WORKGROUP_SIZE_*gl_NumWorkGroups.x;

		if (totalInvocationCount <= BIN_COUNT)
		{
			// need to reuse invocations
			const uint stepCount = uint(ceil(float(BIN_COUNT)/float(totalInvocationCount)));
			for (uint step = 0u; step < stepCount; ++step)
			{
				uint index = gl_GlobalInvocationID.x + step*totalInvocationCount;
				if (index < BIN_COUNT)
					importanceHistogram.data[useHistogramOffset + index] = 0u;
			}
		}
		else
		{
			if (gl_GlobalInvocationID.x < BIN_COUNT)
				importanceHistogram.data[useHistogramOffset + gl_GlobalInvocationID.x] = 0u;
		}
	}
	
	uint budgetIntersectionRecordCapacity = MEMORY_BUDGET/8u;
	const uint marginIntersectionRecordCount = uint(BUDGETING_MARGIN * budgetIntersectionRecordCapacity);

	// Note(achal): Can't reuse the code in the algorithms header because:
	//		1. Not very reusable as is
	//		2. No way to supply the cumulative histogram stored in shared memory
	// so, repurposing the code instead.

	const uint binsToDiscard = (pc.lightCount < marginIntersectionRecordCount) ? 0u : (pc.lightCount - marginIntersectionRecordCount);
	float threshold;
	{
		uint begin = 0u;
		uint end = BIN_COUNT;
		const uint value = binsToDiscard;

		const int minVal = floatBitsToInt(MIN_HISTOGRAM_IMPORTANCE) + 1;
		const int maxVal = floatBitsToInt(MAX_HISTOGRAM_IMPORTANCE) - 1;
		const int range = maxVal - minVal;

		uint len = end-begin;
		if (NBL_GLSL_IS_NOT_POT(len))
		{
			const uint newLen = 0x1u<<findMSB(len);
			const uint diff = len-newLen;

			begin = NBL_GLSL_LESS(NBL_GLSL_EVAL(cumHistogramShared)[newLen],value) ? diff:0u;
			len = newLen;
		}
		while (len!=0u)
		{
			begin += NBL_GLSL_LESS(NBL_GLSL_EVAL(cumHistogramShared)[begin+(len>>=1u)],value) ? len:0u;
			begin += NBL_GLSL_LESS(NBL_GLSL_EVAL(cumHistogramShared)[begin+(len>>=1u)],value) ? len:0u;
		}
		const uint thresholdBinIndex = begin+(NBL_GLSL_LESS(NBL_GLSL_EVAL(cumHistogramShared)[begin],value) ? 1u:0u);
		const uint binMinBitPattern = uint(thresholdBinIndex * (float(range) / float(BIN_COUNT)) + minVal);
		threshold = uintBitsToFloat(binMinBitPattern); // this needs to be set gradually, not at once, to avoid temporal flickering
	}

	const uint persistentWGStepWidth = gl_NumWorkGroups.x * LIGHTS_PER_WORKGROUP;
	const uint persistentWGStepCount = uint(ceil(float(pc.lightCount)/float(persistentWGStepWidth)));
	for (uint step = 0u; step < persistentWGStepCount; ++step)
	{
		const uint index = step*persistentWGStepWidth + gl_WorkGroupID.x*LIGHTS_PER_WORKGROUP + gl_LocalInvocationIndex;

		if (gl_LocalInvocationIndex < LIGHTS_PER_WORKGROUP)
		{
			// Todo(achal): I MIGHT not have to do this once I switch to a usamplerBuffer (and consequently texelFetch) for activeLightIndices
			if (index < pc.lightCount)
				scratchShared[gl_LocalInvocationIndex] = activeLightIndices.data[index];
		}

		if (gl_LocalInvocationIndex == LIGHTS_PER_WORKGROUP)
			scratchShared[lastSharedUint] = 0u;
		barrier();

		const uint localLightIndex = gl_LocalInvocationIndex/INVOCATIONS_PER_LIGHT;
		const uint globalLightIndex = scratchShared[localLightIndex];
		barrier();

		if (index < pc.lightCount)
		{
			// flow no longer uniform within workgroup, careful with barriers
			
			const nbl_glsl_ext_ClusteredLighting_SpotLight light = lights.data[globalLightIndex];
			const float importanceMagnitude = getLightImportanceMagnitude(light);

			// if (importanceMagnitude < threshold)
			// 	break;

			const uvec3 localClusterID = (uvec3(gl_LocalInvocationIndex)>>uvec3(0,1,2))&0x1u; // Hardcoded INVOCATIONS_PER_LIGHT=8u, extensions are trivial
				
			const cone_t cone = getLightVolume(light);
			const float voxelSideLength = CLIPMAP_EXTENT/2;

			const vec3 levelMinVertex = vec3(-CLIPMAP_EXTENT/2.f); // this will be same for all levels of the octree
			const nbl_glsl_shapes_AABB_t cluster = getCluster(localClusterID, levelMinVertex, voxelSideLength);

			if (coneIntersectAABB(cone, cluster))
			{
				const uint localOffset = atomicAdd(scratchShared[lastSharedUint], 1u);

				// repack outputs for coalesced write
				intersection_record_t record;
				record.localClusterID = localClusterID;
				record.level = 1u;
				record.localLightIndex = 0xFFF; // for first cull, this can be garbage, for the last cull, this will be derived by doing imageAtomicAdd on the lightGrid
				record.globalLightIndex = globalLightIndex;
				const uvec2 packed = packIntersectionRecord(record);

				scratchShared[localOffset] = packed.x;
				scratchShared[localOffset+_NBL_GLSL_WORKGROUP_SIZE_] = packed.y;
			}
		}
		barrier();

		// each light can spawn `INVOCATIONS_PER_LIGHT` copies of itself (which are found to be intersecting the children light grid nodes of the node the current light reference is referencing)
		const uint lightReferencesSpawned = scratchShared[lastSharedUint];
		const bool invocationWillWrite = gl_LocalInvocationIndex<lightReferencesSpawned;

		// elect one invocation && best to avoid a +0 on an atomic
		if (gl_LocalInvocationIndex==0u && invocationWillWrite)
			scratchShared[lastSharedUint+1u] = atomicAdd(outScratch.count, lightReferencesSpawned);
		barrier();

		if (invocationWillWrite)
		{
			const uint baseOffset = scratchShared[lastSharedUint+1u];
			const uint outIndex = baseOffset+gl_LocalInvocationIndex;
			if (outIndex < budgetIntersectionRecordCapacity)
				outScratch.data[outIndex] = uvec2(scratchShared[gl_LocalInvocationIndex],scratchShared[gl_LocalInvocationIndex+_NBL_GLSL_WORKGROUP_SIZE_]);
		}
	}

	
#if 0
	// workgroup uniform loop (important for the barrier call)
	for (uint baseLight = gl_WorkGroupID.x*LIGHTS_PER_WORKGROUP; baseLight < pc.lightCount; baseLight += persistentWGStep)
	{
		if (gl_LocalInvocationIndex < LIGHTS_PER_WORKGROUP)
			// scratchShared[gl_LocalInvocationIndex] = texelFetch(activeLightList,baseLight+gl_LocalInvocationIndex)[0];
			scratchShared[gl_LocalInvocationIndex] = activeLightIndices.data[baseLight + gl_LocalInvocationIndex];

		if (gl_LocalInvocationIndex == LIGHTS_PER_WORKGROUP)
			scratchShared[lastSharedUint] = 0u;
		barrier();

		const uint localLight = gl_LocalInvocationIndex/INVOCATIONS_PER_LIGHT;
		const uint globalLightIndex = scratchShared[localLight];
		barrier();

		// finer out of bounds check
		if (baseLight + localLight < pc.lightCount)
		{
			// flow no longer uniform within workgroup, careful with barriers

			const nbl_glsl_ext_ClusteredLighting_SpotLight light = lights.data[globalLightIndex];
			const float importanceMagnitude = getLightImportanceMagnitude(light);

			if (importanceMagnitude < threshold)
				break;

			const uvec3 localClusterID = (uvec3(gl_LocalInvocationIndex)>>uvec3(0,1,2))&0x1u; // Hardcoded INVOCATIONS_PER_LIGHT=8u, extensions are trivial
				
			const cone_t cone = getLightVolume(light);
			const float voxelSideLength = CLIPMAP_EXTENT/2.f;

			const vec3 levelMinVertex = vec3(-CLIPMAP_EXTENT/2.f); // this will be same for all levels of the octree
			const nbl_glsl_shapes_AABB_t cluster = getCluster(localClusterID, levelMinVertex, voxelSideLength);

			if (coneIntersectAABB(cone, cluster))
			{
				const uint localOffset = atomicAdd(scratchShared[lastSharedUint], 1u);

				// repack outputs for coalesced write
				intersection_record_t record;
				record.localClusterID = localClusterID;
				record.level = 1u;
				record.localLightIndex = 0xFFF; // for first cull, this can be garbage, for the last cull, this will be derived by doing imageAtomicAdd on the lightGrid
				record.globalLightIndex = globalLightIndex;
				const uvec2 packed = packIntersectionRecord(record);

				scratchShared[localOffset] = packed.x;
				scratchShared[localOffset+_NBL_GLSL_WORKGROUP_SIZE_] = packed.y;
			}
		}
		barrier();

		// each light can spawn `INVOCATIONS_PER_LIGHT` copies of itself (which are found to be intersecting the children light grid nodes of the node the current light reference is referencing)
		const uint lightReferencesSpawned = scratchShared[lastSharedUint];
		const bool invocationWillWrite = gl_LocalInvocationIndex<lightReferencesSpawned;

		// elect one invocation && best to avoid a +0 on an atomic
		if (gl_LocalInvocationIndex==0u && invocationWillWrite)
			scratchShared[lastSharedUint+1u] = atomicAdd(outScratch.count, lightReferencesSpawned);
		barrier();

		if (invocationWillWrite)
		{
			const uint baseOffset = scratchShared[lastSharedUint+1u];
			outScratch.data[baseOffset+gl_LocalInvocationIndex] = uvec2(scratchShared[gl_LocalInvocationIndex],scratchShared[gl_LocalInvocationIndex+_NBL_GLSL_WORKGROUP_SIZE_]);
		}
	}
#endif

}
